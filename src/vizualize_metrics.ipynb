{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2889aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# ======================\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# This notebook is in src/, so root is \"..\"\n",
    "ROOT = Path(\"..\")\n",
    "\n",
    "# Local imports\n",
    "from dataset import SatelliteSegDataset\n",
    "from unet import UNet\n",
    "from metrics import confusion_matrix, iou_from_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a937744",
   "metadata": {},
   "source": [
    "# Training Metrics Visualization Notebook\n",
    "\n",
    "This notebook evaluates model performance on train/test splits:\n",
    "- Compute confusion matrices\n",
    "- Calculate per-class metrics (IoU, Dice, Precision, Recall, F1)\n",
    "- Visualize confusion matrices and per-class performance\n",
    "- Compare predictions with ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe37ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# CONFIGURATION\n",
    "# ======================\n",
    "\n",
    "# Model checkpoint path\n",
    "CKPT_PATH = ROOT / \"outputs\" / \"best_unet.pth\"\n",
    "\n",
    "# Dataset split to evaluate\n",
    "SPLIT = \"test\"  # \"train\" or \"test\"\n",
    "\n",
    "# DataLoader configuration\n",
    "BATCH_SIZE = 32       # CPU-friendly batch size\n",
    "NUM_WORKERS = 6       # Number of data loading workers (0 on Windows if issues)\n",
    "\n",
    "# Model configuration\n",
    "NUM_CLASSES = 10\n",
    "IGNORE_INDEX = 255\n",
    "IGNORE_LABELS = (0, 1)  # Classes to ignore in metrics\n",
    "\n",
    "# Class names mapping\n",
    "CLASS_NAMES = {\n",
    "    0: \"no_data\",\n",
    "    1: \"clouds\",\n",
    "    2: \"artificial\",\n",
    "    3: \"cultivated\",\n",
    "    4: \"broadleaf\",\n",
    "    5: \"coniferous\",\n",
    "    6: \"herbaceous\",\n",
    "    7: \"natural_soil\",\n",
    "    8: \"permanent_snow\",\n",
    "    9: \"water\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bd369",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "\n",
    "Load evaluation dataset and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49782be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "ds = SatelliteSegDataset(\n",
    "    images_dir=ROOT / \"dataset\" / SPLIT / \"images\",\n",
    "    masks_dir=ROOT / \"dataset\" / SPLIT / \"masks\",\n",
    "    ignore_labels=IGNORE_LABELS,\n",
    "    ignore_index=IGNORE_INDEX\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"======================\")\n",
    "print(\"Dataset configuration:\")\n",
    "print(\"======================\")\n",
    "print(f\"Split: {SPLIT}\")\n",
    "print(f\"Num samples: {len(ds)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Checkpoint: {CKPT_PATH}\")\n",
    "print(f\"✓ Images dir exists: {(ROOT / 'dataset' / SPLIT / 'images').exists()}\")\n",
    "print(f\"✓ Masks dir exists: {(ROOT / 'dataset' / SPLIT / 'masks').exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f01f6",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load trained UNet checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# LOAD MODEL\n",
    "# ======================\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = UNet(in_channels=4, num_classes=NUM_CLASSES, base_channels=32).to(device)\n",
    "state = torch.load(CKPT_PATH, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c829b0",
   "metadata": {},
   "source": [
    "## Metrics Calculation\n",
    "\n",
    "Compute IoU, Dice, Precision, Recall, F1 from confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc4c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_cm(cm: torch.Tensor) -> dict:\n",
    "    \"\"\"\n",
    "    Compute metrics from confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "      - cm: (C, C) int64 confusion matrix\n",
    "      \n",
    "    Returns:\n",
    "      - Dictionary with per-class and global metrics:\n",
    "        - iou_per_class, miou: Intersection over Union\n",
    "        - acc_global: Global pixel accuracy\n",
    "        - precision, recall, f1: Per-class metrics\n",
    "        - dice: Dice coefficient per class\n",
    "        - support: Ground truth pixel count per class\n",
    "    \"\"\"\n",
    "    cm = cm.to(torch.float32)\n",
    "\n",
    "    tp = torch.diag(cm)                      # True positives\n",
    "    fp = cm.sum(0) - tp                      # False positives\n",
    "    fn = cm.sum(1) - tp                      # False negatives\n",
    "    tn = cm.sum() - (tp + fp + fn)           # True negatives\n",
    "\n",
    "    # -------- IoU --------\n",
    "    denom_iou = tp + fp + fn\n",
    "    iou = torch.where(denom_iou > 0, tp / denom_iou, torch.zeros_like(denom_iou))\n",
    "    valid = denom_iou > 0\n",
    "    miou = iou[valid].mean() if valid.any() else torch.tensor(0.0)\n",
    "\n",
    "    # -------- Global Accuracy --------\n",
    "    acc_global = tp.sum() / (cm.sum() + 1e-12)\n",
    "\n",
    "    # -------- Precision, Recall, F1 --------\n",
    "    precision = torch.where((tp + fp) > 0, tp / (tp + fp), torch.zeros_like(tp))\n",
    "    recall = torch.where((tp + fn) > 0, tp / (tp + fn), torch.zeros_like(tp))\n",
    "    f1 = torch.where((precision + recall) > 0, 2 * precision * recall / (precision + recall), torch.zeros_like(tp))\n",
    "\n",
    "    # -------- Dice --------\n",
    "    dice = torch.where((2*tp + fp + fn) > 0, (2*tp) / (2*tp + fp + fn), torch.zeros_like(tp))\n",
    "\n",
    "    # -------- Support --------\n",
    "    support = cm.sum(1)  # Ground truth pixels per class\n",
    "\n",
    "    return {\n",
    "        \"iou\": iou,\n",
    "        \"miou\": miou,\n",
    "        \"acc_global\": acc_global,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"dice\": dice,\n",
    "        \"support\": support\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd60cd",
   "metadata": {},
   "source": [
    "## Compute Confusion Matrix\n",
    "\n",
    "Run inference on entire dataset to build confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# COMPUTE CONFUSION MATRIX\n",
    "# ======================\n",
    "\n",
    "cm_total = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64)\n",
    "\n",
    "print(f\"Computing confusion matrix on {SPLIT} split...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)                           # (B, C, H, W)\n",
    "        pred = torch.argmax(logits, dim=1)          # (B, H, W)\n",
    "\n",
    "        cm_total += confusion_matrix(\n",
    "            pred, y,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            ignore_index=IGNORE_INDEX\n",
    "        )\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {(batch_idx + 1) * BATCH_SIZE} samples...\")\n",
    "\n",
    "print(f\"✓ Confusion matrix computed: {cm_total.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f9bbb",
   "metadata": {},
   "source": [
    "## Per-Class Metrics Report\n",
    "\n",
    "Display metrics for all classes in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d67f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# CALCULATE METRICS\n",
    "# ======================\n",
    "\n",
    "m = metrics_from_cm(cm_total)\n",
    "\n",
    "# Build results dataframe\n",
    "rows = []\n",
    "for c in range(NUM_CLASSES):\n",
    "    rows.append({\n",
    "        \"class\": c,\n",
    "        \"name\": CLASS_NAMES.get(c, f\"class_{c}\"),\n",
    "        \"support_pixels\": int(m[\"support\"][c].item()),\n",
    "        \"IoU\": float(m[\"iou\"][c].item()),\n",
    "        \"Dice\": float(m[\"dice\"][c].item()),\n",
    "        \"Precision\": float(m[\"precision\"][c].item()),\n",
    "        \"Recall\": float(m[\"recall\"][c].item()),\n",
    "        \"F1\": float(m[\"f1\"][c].item()),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Print global metrics\n",
    "print(\"======================\")\n",
    "print(\"GLOBAL METRICS\")\n",
    "print(\"======================\")\n",
    "print(f\"Pixel Accuracy: {float(m['acc_global'].item()):.4f}\")\n",
    "print(f\"Mean IoU (mIoU): {float(m['miou'].item()):.4f}\")\n",
    "\n",
    "print(\"\\nPer-class metrics (sorted by support):\")\n",
    "print(df.sort_values(\"support_pixels\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeef817",
   "metadata": {},
   "source": [
    "## Filtered Metrics\n",
    "\n",
    "Optionally analyze metrics for well-represented classes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e55a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# FILTERED METRICS (exclude ignored classes)\n",
    "# ======================\n",
    "\n",
    "ignore_report = {0, 1}  # Classes to ignore in metrics\n",
    "\n",
    "valid_classes = [c for c in range(NUM_CLASSES) if c not in ignore_report]\n",
    "iou_vals = m[\"iou\"][valid_classes]\n",
    "support_vals = m[\"support\"][valid_classes]\n",
    "\n",
    "# Compute mIoU for valid classes only (those with pixels)\n",
    "valid_mask = support_vals > 0\n",
    "miou_no_ignored = iou_vals[valid_mask].mean() if valid_mask.any() else torch.tensor(0.0)\n",
    "\n",
    "print(\"======================\")\n",
    "print(\"FILTERED METRICS\")\n",
    "print(\"======================\")\n",
    "print(f\"mIoU (excluding classes {ignore_report}, only present classes):\")\n",
    "print(f\"  {float(miou_no_ignored.item()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8efa4a",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization\n",
    "\n",
    "Display confusion matrices with different normalization views."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e16cfe",
   "metadata": {},
   "source": [
    "Confusion Matrix like slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, normalize=None, title=\"Confusion Matrix\", max_classes=None):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrix with optional normalization.\n",
    "    \n",
    "    Parameters:\n",
    "      - cm: (C, C) confusion matrix (torch.Tensor or np.ndarray)\n",
    "      - class_names: List of class name strings\n",
    "      - normalize: None | \"recall\" | \"precision\"\n",
    "        - None: raw counts\n",
    "        - \"recall\": normalize by ground truth totals (row-wise)\n",
    "        - \"precision\": normalize by predicted totals (column-wise)\n",
    "      - max_classes: Display top N classes by support (for readability)\n",
    "    \"\"\"\n",
    "    if hasattr(cm, \"cpu\"):\n",
    "        cm = cm.cpu().numpy()\n",
    "    cm = cm.astype(np.float64)\n",
    "\n",
    "    C = cm.shape[0]\n",
    "    idx = list(range(C))\n",
    "    \n",
    "    # Filter to top classes by support if requested\n",
    "    if max_classes is not None and max_classes < C:\n",
    "        support = cm.sum(axis=1)\n",
    "        idx = np.argsort(-support)[:max_classes].tolist()\n",
    "        cm = cm[np.ix_(idx, idx)]\n",
    "        class_names = [class_names[i] for i in idx]\n",
    "\n",
    "    # Apply normalization\n",
    "    if normalize == \"recall\":\n",
    "        # Normalize by row (ground truth totals)\n",
    "        denom = cm.sum(axis=1, keepdims=True)\n",
    "        cm_show = np.divide(cm, denom, out=np.zeros_like(cm), where=denom != 0)\n",
    "        fmt = \"{:.2f}\"\n",
    "        subtitle = \" (normalized by GT: recall view)\"\n",
    "    elif normalize == \"precision\":\n",
    "        # Normalize by column (predicted totals)\n",
    "        denom = cm.sum(axis=0, keepdims=True)\n",
    "        cm_show = np.divide(cm, denom, out=np.zeros_like(cm), where=denom != 0)\n",
    "        fmt = \"{:.2f}\"\n",
    "        subtitle = \" (normalized by Pred: precision view)\"\n",
    "    else:\n",
    "        cm_show = cm\n",
    "        fmt = \"{:.0f}\"\n",
    "        subtitle = \" (counts)\"\n",
    "\n",
    "    # Create figure\n",
    "    fig_w = max(7, 0.7 * len(class_names))\n",
    "    fig_h = max(6, 0.7 * len(class_names))\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    \n",
    "    plt.imshow(cm_show, cmap=\"Blues\")\n",
    "    plt.title(title + subtitle, fontsize=12)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Labels\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(class_names)), class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Ground Truth\")\n",
    "\n",
    "    # Annotate cells\n",
    "    for i in range(cm_show.shape[0]):\n",
    "        for j in range(cm_show.shape[1]):\n",
    "            val = cm_show[i, j]\n",
    "            text = fmt.format(val)\n",
    "            plt.text(j, i, text, ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56720d00",
   "metadata": {},
   "source": [
    "## Confusion Matrix Plots\n",
    "\n",
    "Three views of the confusion matrix: raw counts, recall-normalized, and precision-normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build class name list in index order\n",
    "class_names = [CLASS_NAMES.get(i, f\"class_{i}\") for i in range(NUM_CLASSES)]\n",
    "\n",
    "# Plot three views\n",
    "plot_confusion_matrix(cm_total, class_names, normalize=None, \n",
    "                     title=f\"Confusion Matrix ({SPLIT})\")\n",
    "plot_confusion_matrix(cm_total, class_names, normalize=\"recall\", \n",
    "                     title=f\"Confusion Matrix ({SPLIT})\")\n",
    "plot_confusion_matrix(cm_total, class_names, normalize=\"precision\", \n",
    "                     title=f\"Confusion Matrix ({SPLIT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead48adb",
   "metadata": {},
   "source": [
    "## Metrics Heatmap\n",
    "\n",
    "Visualize per-class metrics (IoU, Dice, etc.) as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b795fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_heatmap(df_metrics, class_col=\"name\", \n",
    "                         metrics=(\"IoU\", \"Dice\", \"Precision\", \"Recall\", \"F1\"),\n",
    "                         title=\"Per-class metrics\", sort_by=\"support_pixels\"):\n",
    "    \"\"\"\n",
    "    Visualize per-class metrics as a heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "      - df_metrics: DataFrame with class metrics\n",
    "      - class_col: Column name for class labels\n",
    "      - metrics: Tuple of metric columns to display\n",
    "      - sort_by: Column to sort classes (default: support)\n",
    "    \"\"\"\n",
    "    d = df_metrics.copy()\n",
    "    if sort_by in d.columns:\n",
    "        d = d.sort_values(sort_by, ascending=False)\n",
    "\n",
    "    classes = d[class_col].tolist()\n",
    "    M = d[list(metrics)].to_numpy(dtype=float)\n",
    "\n",
    "    # Create heatmap\n",
    "    fig_w = 1.2 * len(metrics) + 6\n",
    "    fig_h = 0.45 * len(classes) + 2\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    \n",
    "    plt.imshow(M, cmap=\"RdYlGn\", aspect=\"auto\", vmin=0, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Labels\n",
    "    plt.xticks(range(len(metrics)), metrics, rotation=0)\n",
    "    plt.yticks(range(len(classes)), classes)\n",
    "\n",
    "    # Annotate cells\n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            plt.text(j, i, f\"{M[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display metrics heatmap\n",
    "plot_metrics_heatmap(df, title=f\"Per-class metrics ({SPLIT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14a86b",
   "metadata": {},
   "source": [
    "## Filtered Class Analysis\n",
    "\n",
    "Analyze metrics for well-represented classes (with sufficient support)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b57f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter classes by minimum pixel support\n",
    "MIN_SUPPORT = 50_000  # Adjust this threshold as needed\n",
    "df_filt = df[df[\"support_pixels\"] >= MIN_SUPPORT]\n",
    "\n",
    "print(f\"\\nClasses with support >= {MIN_SUPPORT} pixels:\")\n",
    "print(df_filt.sort_values(\"support_pixels\", ascending=False).to_string(index=False))\n",
    "\n",
    "# Visualize filtered metrics\n",
    "if len(df_filt) > 0:\n",
    "    plot_metrics_heatmap(df_filt, title=f\"Per-class metrics (support ≥ {MIN_SUPPORT}, {SPLIT})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b78140e",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Optionally save confusion matrix and metrics for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba48948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# SAVE RESULTS\n",
    "# ======================\n",
    "\n",
    "out_dir = ROOT / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save confusion matrix\n",
    "cm_path = out_dir / f\"confusion_matrix_{SPLIT}.pt\"\n",
    "torch.save(cm_total.cpu(), cm_path)\n",
    "print(f\"✓ Saved confusion matrix: {cm_path}\")\n",
    "\n",
    "# Save metrics report\n",
    "csv_path = out_dir / f\"metrics_{SPLIT}.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Saved metrics report: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc8234",
   "metadata": {},
   "source": [
    "## Load Previously Computed Results\n",
    "\n",
    "Load confusion matrix from a previous run (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a previously saved confusion matrix\n",
    "# cm_loaded = torch.load(\"../outputs/confusion_matrix_test.pt\")\n",
    "print(\"✓ Cell ready to load saved confusion matrix (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62332fcb",
   "metadata": {},
   "source": [
    "## Epoch-by-Epoch Confusion Matrices\n",
    "\n",
    "Load and analyze confusion matrices saved during training (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66764e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load confusion matrix from training epoch\n",
    "# cm_epoch = torch.load(\"../outputs/val_cm_epoch_05.pt\")\n",
    "# print(f\"Shape: {cm_epoch.shape}\")\n",
    "\n",
    "print(\"✓ Ready to load epoch confusion matrices (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute IoU metrics from loaded confusion matrix\n",
    "# from metrics import iou_from_cm\n",
    "# iou_per_class, miou = iou_from_cm(cm_epoch)\n",
    "# print(f\"mIoU: {float(miou):.4f}\")\n",
    "# for c, v in enumerate(iou_per_class):\n",
    "#     print(f\"  Class {c}: IoU = {float(v):.4f}\")\n",
    "\n",
    "print(\"✓ Ready to compute metrics from saved confusion matrix (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e1eb6d",
   "metadata": {},
   "source": [
    "## Reference: Example Results\n",
    "\n",
    "Example metrics from training runs (for reference):\n",
    "\n",
    "**Training 2 (4 epochs):**\n",
    "- mIoU: 0.4757\n",
    "- Class 2 (artificial): 0.5818\n",
    "- Class 3 (cultivated): 0.6628\n",
    "- Class 4 (broadleaf): 0.7099\n",
    "- Class 9 (water): 0.6942\n",
    "\n",
    "**Training 1 (2 epochs):**\n",
    "- mIoU: 0.4461\n",
    "- Class 9 (water): 0.7682\n",
    "- Class 4 (broadleaf): 0.6776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fcb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ Metrics visualization notebook complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
